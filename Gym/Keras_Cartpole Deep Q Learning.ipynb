{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate=0.01, state_size=4,\n",
    "                 action_size=2, hidden_size=10):\n",
    "        # state inputs to the Q-network\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Dense(hidden_size, activation='relu',\n",
    "                             input_dim=state_size))\n",
    "        self.model.add(Dense(hidden_size, activation='relu'))\n",
    "        self.model.add(Dense(action_size, activation='linear'))\n",
    "\n",
    "        self.optimizer = Adam(lr=learning_rate)\n",
    "        self.model.compile(loss='mse', optimizer=self.optimizer)\n",
    "\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, max_size=1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size,\n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "train_episodes = 1000          # max number of episodes to learn from\n",
    "max_steps = 200                # max steps in an episode\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability\n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 16               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 10000            # memory capacity\n",
    "batch_size = 32                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "\n",
    "# Create the Cart-Pole game environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# network setup\n",
    "mainQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "###################################\n",
    "## Populate the experience memory\n",
    "###################################\n",
    "\n",
    "# Initialize the simulation\n",
    "env.reset()\n",
    "# Take one random step to get the pole and cart moving\n",
    "state, reward, done, _ = env.step(env.action_space.sample())\n",
    "state = np.reshape(state, [1, 4])\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make a bunch of random actions and store the experiences\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    # env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, 4])\n",
    "\n",
    "    if done:\n",
    "        # The simulation fails so no next state\n",
    "        next_state = np.zeros(state.shape)\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "\n",
    "        # Start new episode\n",
    "        env.reset()\n",
    "        # Take one random step to get the pole and cart moving\n",
    "        state, reward, done, _ = env.step(env.action_space.sample())\n",
    "        state = np.reshape(state, [1, 4])\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Total reward: 2.0 Explore P: 0.9998\n",
      "Episode: 2 Total reward: 14.0 Explore P: 0.9984\n",
      "Episode: 3 Total reward: 19.0 Explore P: 0.9965\n",
      "Episode: 4 Total reward: 11.0 Explore P: 0.9955\n",
      "Episode: 5 Total reward: 11.0 Explore P: 0.9944\n",
      "Episode: 6 Total reward: 18.0 Explore P: 0.9926\n",
      "Episode: 7 Total reward: 16.0 Explore P: 0.9910\n",
      "Episode: 8 Total reward: 13.0 Explore P: 0.9898\n",
      "Episode: 9 Total reward: 12.0 Explore P: 0.9886\n",
      "Episode: 10 Total reward: 27.0 Explore P: 0.9859\n",
      "Episode: 11 Total reward: 12.0 Explore P: 0.9848\n",
      "Episode: 12 Total reward: 18.0 Explore P: 0.9830\n",
      "Episode: 13 Total reward: 41.0 Explore P: 0.9790\n",
      "Episode: 14 Total reward: 23.0 Explore P: 0.9768\n",
      "Episode: 15 Total reward: 24.0 Explore P: 0.9745\n",
      "Episode: 16 Total reward: 12.0 Explore P: 0.9733\n",
      "Episode: 17 Total reward: 43.0 Explore P: 0.9692\n",
      "Episode: 18 Total reward: 32.0 Explore P: 0.9661\n",
      "Episode: 19 Total reward: 25.0 Explore P: 0.9638\n",
      "Episode: 20 Total reward: 13.0 Explore P: 0.9625\n",
      "Episode: 21 Total reward: 13.0 Explore P: 0.9613\n",
      "Episode: 22 Total reward: 19.0 Explore P: 0.9595\n",
      "Episode: 23 Total reward: 11.0 Explore P: 0.9584\n",
      "Episode: 24 Total reward: 26.0 Explore P: 0.9560\n",
      "Episode: 25 Total reward: 44.0 Explore P: 0.9518\n",
      "Episode: 26 Total reward: 24.0 Explore P: 0.9496\n",
      "Episode: 27 Total reward: 14.0 Explore P: 0.9482\n",
      "Episode: 28 Total reward: 11.0 Explore P: 0.9472\n",
      "Episode: 29 Total reward: 23.0 Explore P: 0.9451\n",
      "Episode: 30 Total reward: 55.0 Explore P: 0.9399\n",
      "Episode: 31 Total reward: 36.0 Explore P: 0.9366\n",
      "Episode: 32 Total reward: 19.0 Explore P: 0.9348\n",
      "Episode: 33 Total reward: 16.0 Explore P: 0.9333\n",
      "Episode: 34 Total reward: 17.0 Explore P: 0.9318\n",
      "Episode: 35 Total reward: 26.0 Explore P: 0.9294\n",
      "Episode: 36 Total reward: 15.0 Explore P: 0.9280\n",
      "Episode: 37 Total reward: 19.0 Explore P: 0.9263\n",
      "Episode: 38 Total reward: 24.0 Explore P: 0.9241\n",
      "Episode: 39 Total reward: 27.0 Explore P: 0.9216\n",
      "Episode: 40 Total reward: 12.0 Explore P: 0.9205\n",
      "Episode: 41 Total reward: 33.0 Explore P: 0.9175\n",
      "Episode: 42 Total reward: 22.0 Explore P: 0.9155\n",
      "Episode: 43 Total reward: 30.0 Explore P: 0.9128\n",
      "Episode: 44 Total reward: 11.0 Explore P: 0.9118\n",
      "Episode: 45 Total reward: 20.0 Explore P: 0.9100\n",
      "Episode: 46 Total reward: 20.0 Explore P: 0.9082\n",
      "Episode: 47 Total reward: 55.0 Explore P: 0.9033\n",
      "Episode: 48 Total reward: 49.0 Explore P: 0.8989\n",
      "Episode: 49 Total reward: 44.0 Explore P: 0.8950\n",
      "Episode: 50 Total reward: 16.0 Explore P: 0.8936\n",
      "Episode: 51 Total reward: 20.0 Explore P: 0.8918\n",
      "Episode: 52 Total reward: 16.0 Explore P: 0.8904\n",
      "Episode: 53 Total reward: 23.0 Explore P: 0.8884\n",
      "Episode: 54 Total reward: 57.0 Explore P: 0.8834\n",
      "Episode: 55 Total reward: 19.0 Explore P: 0.8818\n",
      "Episode: 56 Total reward: 11.0 Explore P: 0.8808\n",
      "Episode: 57 Total reward: 35.0 Explore P: 0.8778\n",
      "Episode: 58 Total reward: 62.0 Explore P: 0.8724\n",
      "Episode: 59 Total reward: 14.0 Explore P: 0.8712\n",
      "Episode: 60 Total reward: 32.0 Explore P: 0.8684\n",
      "Episode: 61 Total reward: 23.0 Explore P: 0.8665\n",
      "Episode: 62 Total reward: 20.0 Explore P: 0.8647\n",
      "Episode: 63 Total reward: 19.0 Explore P: 0.8631\n",
      "Episode: 64 Total reward: 11.0 Explore P: 0.8622\n",
      "Episode: 65 Total reward: 16.0 Explore P: 0.8608\n",
      "Episode: 66 Total reward: 71.0 Explore P: 0.8548\n",
      "Episode: 67 Total reward: 12.0 Explore P: 0.8538\n",
      "Episode: 68 Total reward: 50.0 Explore P: 0.8496\n",
      "Episode: 69 Total reward: 13.0 Explore P: 0.8485\n",
      "Episode: 70 Total reward: 33.0 Explore P: 0.8457\n",
      "Episode: 71 Total reward: 14.0 Explore P: 0.8446\n",
      "Episode: 72 Total reward: 23.0 Explore P: 0.8426\n",
      "Episode: 73 Total reward: 57.0 Explore P: 0.8379\n",
      "Episode: 74 Total reward: 25.0 Explore P: 0.8358\n",
      "Episode: 75 Total reward: 14.0 Explore P: 0.8347\n",
      "Episode: 76 Total reward: 13.0 Explore P: 0.8336\n",
      "Episode: 77 Total reward: 40.0 Explore P: 0.8303\n",
      "Episode: 78 Total reward: 13.0 Explore P: 0.8293\n",
      "Episode: 79 Total reward: 8.0 Explore P: 0.8286\n",
      "Episode: 80 Total reward: 11.0 Explore P: 0.8277\n",
      "Episode: 81 Total reward: 16.0 Explore P: 0.8264\n",
      "Episode: 82 Total reward: 20.0 Explore P: 0.8248\n",
      "Episode: 83 Total reward: 54.0 Explore P: 0.8204\n",
      "Episode: 84 Total reward: 16.0 Explore P: 0.8191\n",
      "Episode: 85 Total reward: 74.0 Explore P: 0.8131\n",
      "Episode: 86 Total reward: 47.0 Explore P: 0.8094\n",
      "Episode: 87 Total reward: 28.0 Explore P: 0.8071\n",
      "Episode: 88 Total reward: 43.0 Explore P: 0.8037\n",
      "Episode: 89 Total reward: 12.0 Explore P: 0.8027\n",
      "Episode: 90 Total reward: 41.0 Explore P: 0.7995\n",
      "Episode: 91 Total reward: 106.0 Explore P: 0.7912\n",
      "Episode: 92 Total reward: 43.0 Explore P: 0.7878\n",
      "Episode: 93 Total reward: 17.0 Explore P: 0.7865\n",
      "Episode: 94 Total reward: 21.0 Explore P: 0.7849\n",
      "Episode: 95 Total reward: 15.0 Explore P: 0.7837\n",
      "Episode: 96 Total reward: 23.0 Explore P: 0.7819\n",
      "Episode: 97 Total reward: 24.0 Explore P: 0.7801\n",
      "Episode: 98 Total reward: 62.0 Explore P: 0.7753\n",
      "Episode: 99 Total reward: 43.0 Explore P: 0.7720\n",
      "Episode: 100 Total reward: 43.0 Explore P: 0.7688\n",
      "Episode: 101 Total reward: 23.0 Explore P: 0.7670\n",
      "Episode: 102 Total reward: 15.0 Explore P: 0.7659\n",
      "Episode: 103 Total reward: 67.0 Explore P: 0.7608\n",
      "Episode: 104 Total reward: 21.0 Explore P: 0.7593\n",
      "Episode: 105 Total reward: 30.0 Explore P: 0.7570\n",
      "Episode: 106 Total reward: 46.0 Explore P: 0.7536\n",
      "Episode: 107 Total reward: 49.0 Explore P: 0.7500\n",
      "Episode: 108 Total reward: 10.0 Explore P: 0.7492\n",
      "Episode: 109 Total reward: 30.0 Explore P: 0.7470\n",
      "Episode: 110 Total reward: 29.0 Explore P: 0.7449\n",
      "Episode: 111 Total reward: 15.0 Explore P: 0.7438\n",
      "Episode: 112 Total reward: 15.0 Explore P: 0.7427\n",
      "Episode: 113 Total reward: 23.0 Explore P: 0.7410\n",
      "Episode: 114 Total reward: 56.0 Explore P: 0.7369\n",
      "Episode: 115 Total reward: 33.0 Explore P: 0.7345\n",
      "Episode: 116 Total reward: 22.0 Explore P: 0.7329\n",
      "Episode: 117 Total reward: 41.0 Explore P: 0.7300\n",
      "Episode: 118 Total reward: 60.0 Explore P: 0.7257\n",
      "Episode: 119 Total reward: 29.0 Explore P: 0.7236\n",
      "Episode: 120 Total reward: 18.0 Explore P: 0.7223\n",
      "Episode: 121 Total reward: 26.0 Explore P: 0.7205\n",
      "Episode: 122 Total reward: 24.0 Explore P: 0.7188\n",
      "Episode: 123 Total reward: 65.0 Explore P: 0.7142\n",
      "Episode: 124 Total reward: 17.0 Explore P: 0.7130\n",
      "Episode: 125 Total reward: 27.0 Explore P: 0.7111\n",
      "Episode: 126 Total reward: 50.0 Explore P: 0.7076\n",
      "Episode: 127 Total reward: 39.0 Explore P: 0.7049\n",
      "Episode: 128 Total reward: 22.0 Explore P: 0.7033\n",
      "Episode: 129 Total reward: 23.0 Explore P: 0.7017\n",
      "Episode: 130 Total reward: 10.0 Explore P: 0.7010\n",
      "Episode: 131 Total reward: 127.0 Explore P: 0.6923\n",
      "Episode: 132 Total reward: 17.0 Explore P: 0.6912\n",
      "Episode: 133 Total reward: 59.0 Explore P: 0.6872\n",
      "Episode: 134 Total reward: 43.0 Explore P: 0.6843\n",
      "Episode: 135 Total reward: 14.0 Explore P: 0.6833\n",
      "Episode: 136 Total reward: 13.0 Explore P: 0.6824\n",
      "Episode: 137 Total reward: 13.0 Explore P: 0.6816\n",
      "Episode: 138 Total reward: 78.0 Explore P: 0.6763\n",
      "Episode: 139 Total reward: 30.0 Explore P: 0.6743\n",
      "Episode: 140 Total reward: 19.0 Explore P: 0.6731\n",
      "Episode: 141 Total reward: 31.0 Explore P: 0.6710\n",
      "Episode: 142 Total reward: 11.0 Explore P: 0.6703\n",
      "Episode: 143 Total reward: 36.0 Explore P: 0.6679\n",
      "Episode: 144 Total reward: 73.0 Explore P: 0.6631\n",
      "Episode: 145 Total reward: 99.0 Explore P: 0.6567\n",
      "Episode: 146 Total reward: 16.0 Explore P: 0.6557\n",
      "Episode: 147 Total reward: 108.0 Explore P: 0.6487\n",
      "Episode: 148 Total reward: 79.0 Explore P: 0.6437\n",
      "Episode: 149 Total reward: 32.0 Explore P: 0.6417\n",
      "Episode: 150 Total reward: 75.0 Explore P: 0.6370\n",
      "Episode: 151 Total reward: 57.0 Explore P: 0.6334\n",
      "Episode: 152 Total reward: 85.0 Explore P: 0.6281\n",
      "Episode: 153 Total reward: 69.0 Explore P: 0.6239\n",
      "Episode: 154 Total reward: 56.0 Explore P: 0.6205\n",
      "Episode: 155 Total reward: 49.0 Explore P: 0.6175\n",
      "Episode: 156 Total reward: 26.0 Explore P: 0.6159\n",
      "Episode: 157 Total reward: 187.0 Explore P: 0.6047\n",
      "Episode: 158 Total reward: 36.0 Explore P: 0.6025\n",
      "Episode: 159 Total reward: 39.0 Explore P: 0.6002\n",
      "Episode: 160 Total reward: 188.0 Explore P: 0.5892\n",
      "Episode: 161 Total reward: 26.0 Explore P: 0.5877\n",
      "Episode: 162 Total reward: 100.0 Explore P: 0.5820\n",
      "Episode: 163 Total reward: 105.0 Explore P: 0.5760\n",
      "Episode: 164 Total reward: 91.0 Explore P: 0.5709\n",
      "Episode: 165 Total reward: 199.0 Explore P: 0.5598\n",
      "Episode: 166 Total reward: 69.0 Explore P: 0.5560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 167 Total reward: 186.0 Explore P: 0.5460\n",
      "Episode: 168 Total reward: 20.0 Explore P: 0.5449\n",
      "Episode: 169 Total reward: 37.0 Explore P: 0.5429\n",
      "Episode: 170 Total reward: 32.0 Explore P: 0.5412\n",
      "Episode: 171 Total reward: 57.0 Explore P: 0.5382\n",
      "Episode: 172 Total reward: 170.0 Explore P: 0.5293\n",
      "Episode: 173 Total reward: 53.0 Explore P: 0.5266\n",
      "Episode: 174 Total reward: 176.0 Explore P: 0.5176\n",
      "Episode: 175 Total reward: 194.0 Explore P: 0.5078\n",
      "Episode: 176 Total reward: 157.0 Explore P: 0.5000\n",
      "Episode: 177 Total reward: 63.0 Explore P: 0.4970\n",
      "Episode: 178 Total reward: 183.0 Explore P: 0.4881\n",
      "Episode: 179 Total reward: 82.0 Explore P: 0.4842\n",
      "Episode: 180 Total reward: 176.0 Explore P: 0.4760\n",
      "Episode: 181 Total reward: 199.0 Explore P: 0.4668\n",
      "Episode: 182 Total reward: 32.0 Explore P: 0.4653\n",
      "Episode: 183 Total reward: 144.0 Explore P: 0.4588\n",
      "Episode: 184 Total reward: 172.0 Explore P: 0.4512\n",
      "Episode: 185 Total reward: 101.0 Explore P: 0.4467\n",
      "Episode: 186 Total reward: 199.0 Explore P: 0.4381\n",
      "Episode: 187 Total reward: 132.0 Explore P: 0.4325\n",
      "Episode: 188 Total reward: 111.0 Explore P: 0.4278\n",
      "Episode: 189 Total reward: 199.0 Explore P: 0.4196\n",
      "Episode: 190 Total reward: 68.0 Explore P: 0.4168\n",
      "Episode: 191 Total reward: 36.0 Explore P: 0.4154\n",
      "Episode: 192 Total reward: 199.0 Explore P: 0.4074\n",
      "Episode: 193 Total reward: 199.0 Explore P: 0.3996\n",
      "Episode: 194 Total reward: 199.0 Explore P: 0.3919\n",
      "Episode: 195 Total reward: 199.0 Explore P: 0.3844\n",
      "Episode: 196 Total reward: 184.0 Explore P: 0.3775\n",
      "Episode: 197 Total reward: 38.0 Explore P: 0.3761\n",
      "Episode: 198 Total reward: 199.0 Explore P: 0.3689\n",
      "Episode: 199 Total reward: 199.0 Explore P: 0.3618\n",
      "Episode: 200 Total reward: 199.0 Explore P: 0.3549\n",
      "Episode: 201 Total reward: 199.0 Explore P: 0.3481\n",
      "Episode: 202 Total reward: 178.0 Explore P: 0.3422\n",
      "Episode: 203 Total reward: 199.0 Explore P: 0.3356\n",
      "Episode: 204 Total reward: 102.0 Explore P: 0.3323\n",
      "Episode: 205 Total reward: 151.0 Explore P: 0.3275\n",
      "Episode: 206 Total reward: 199.0 Explore P: 0.3212\n",
      "Episode: 207 Total reward: 199.0 Explore P: 0.3151\n",
      "Episode: 208 Total reward: 199.0 Explore P: 0.3091\n",
      "Episode: 209 Total reward: 199.0 Explore P: 0.3032\n",
      "Episode: 210 Total reward: 199.0 Explore P: 0.2974\n",
      "Episode: 211 Total reward: 199.0 Explore P: 0.2917\n",
      "Episode: 212 Total reward: 199.0 Explore P: 0.2862\n",
      "Episode: 213 Total reward: 199.0 Explore P: 0.2808\n",
      "Episode: 214 Total reward: 199.0 Explore P: 0.2754\n",
      "Episode: 215 Total reward: 199.0 Explore P: 0.2702\n",
      "Episode: 216 Total reward: 199.0 Explore P: 0.2651\n",
      "Episode: 217 Total reward: 199.0 Explore P: 0.2600\n",
      "Episode: 218 Total reward: 199.0 Explore P: 0.2551\n",
      "Episode: 219 Total reward: 199.0 Explore P: 0.2503\n",
      "Episode: 220 Total reward: 199.0 Explore P: 0.2455\n",
      "Episode: 221 Total reward: 199.0 Explore P: 0.2409\n",
      "Episode: 222 Total reward: 199.0 Explore P: 0.2364\n",
      "Episode: 223 Total reward: 199.0 Explore P: 0.2319\n",
      "Episode: 224 Total reward: 199.0 Explore P: 0.2275\n",
      "Episode: 225 Total reward: 176.0 Explore P: 0.2237\n",
      "Episode: 226 Total reward: 199.0 Explore P: 0.2195\n",
      "Episode: 227 Total reward: 199.0 Explore P: 0.2154\n",
      "Episode: 228 Total reward: 199.0 Explore P: 0.2113\n",
      "Episode: 229 Total reward: 189.0 Explore P: 0.2076\n",
      "Episode: 230 Total reward: 199.0 Explore P: 0.2037\n",
      "Episode: 231 Total reward: 199.0 Explore P: 0.1999\n",
      "Episode: 232 Total reward: 199.0 Explore P: 0.1961\n",
      "Episode: 233 Total reward: 199.0 Explore P: 0.1925\n",
      "Episode: 234 Total reward: 199.0 Explore P: 0.1889\n",
      "Episode: 235 Total reward: 199.0 Explore P: 0.1853\n",
      "Episode: 236 Total reward: 199.0 Explore P: 0.1819\n",
      "Episode: 237 Total reward: 199.0 Explore P: 0.1785\n",
      "Episode: 238 Total reward: 199.0 Explore P: 0.1752\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "## Training\n",
    "#############\n",
    "step = 0\n",
    "for ep in range(1, train_episodes):\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    while t < max_steps:\n",
    "        step += 1\n",
    "        # Uncomment this next line to watch the training\n",
    "        # env.render()\n",
    "\n",
    "        # Explore or Exploit\n",
    "        explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step)\n",
    "        if explore_p > np.random.rand():\n",
    "            # Make a random action\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Get action from Q-network\n",
    "            Qs = mainQN.model.predict(state)[0]\n",
    "            action = np.argmax(Qs)\n",
    "\n",
    "        # Take action, get new state and reward\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            # the episode ends so no next state\n",
    "            next_state = np.zeros(state.shape)\n",
    "            t = max_steps\n",
    "\n",
    "            print('Episode: {}'.format(ep),\n",
    "                  'Total reward: {}'.format(total_reward),\n",
    "                  'Explore P: {:.4f}'.format(explore_p))\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "\n",
    "            # Start new episode\n",
    "            env.reset()\n",
    "            # Take one random step to get the pole and cart moving\n",
    "            state, reward, done, _ = env.step(env.action_space.sample())\n",
    "            state = np.reshape(state, [1, 4])\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state))\n",
    "            state = next_state\n",
    "            t += 1\n",
    "\n",
    "        # Replay\n",
    "        inputs = np.zeros((batch_size, 4))\n",
    "        targets = np.zeros((batch_size, 2))\n",
    "\n",
    "        minibatch = memory.sample(batch_size)\n",
    "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(minibatch):\n",
    "            inputs[i:i+1] = state_b\n",
    "            target = reward_b\n",
    "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
    "                target_Q = mainQN.model.predict(next_state_b)[0]\n",
    "                target = reward_b + gamma * np.amax(mainQN.model.predict(next_state_b)[0])\n",
    "            targets[i] = mainQN.model.predict(state_b)\n",
    "            targets[i][action_b] = target\n",
    "        mainQN.model.fit(inputs, targets, epochs=1, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
